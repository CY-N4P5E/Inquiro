#!/usr/bin/env python3
"""
Quick setup script for optimal Inquiro configuration on 13900HX + 16GB RAM + RTX 4060 8GB system.

This script sets the optimal configuration values for your specific hardware
and verifies that everything is working correctly.
"""

import os
import sys
from pathlib import Path

def set_optimal_config():
    """Set optimal configuration values for the user's hardware."""
    
    optimal_config = {
        # Memory management - 11GB limit (leaving 5GB for OS)
        'DEFAULT_MEMORY_LIMIT': '11000',
        
        # Text processing - balanced for quality and performance
        'CHUNK_SIZE': '800',
        'CHUNK_OVERLAP': '80',
        
        # Query settings - optimized for quality
        'DEFAULT_K': '7',
        'DEFAULT_SCORE_THRESHOLD': '0.4',
        'DEFAULT_MAX_CONTEXT_LENGTH': '6000',
        
        # Models - optimized for 8GB VRAM
        'OLLAMA_QUERY_MODEL': 'llama3.1:8b',
        'OLLAMA_EMBEDDING_MODEL': 'nomic-embed-text'
    }
    
    print("üöÄ Setting up optimal configuration for your system...")
    print("   CPU: Intel Core i9-13900HX")
    print("   RAM: 16GB")
    print("   GPU: RTX 4060 8GB VRAM")
    print()
    
    # Set environment variables for current session
    for key, value in optimal_config.items():
        os.environ[key] = value
        print(f"   ‚úì {key} = {value}")
    
    print("\n‚úÖ Configuration applied to current session!")
    return optimal_config

def create_env_file(config):
    """Create a .env file with the optimal configuration."""
    env_file = Path('.env')
    
    print(f"\nüìù Creating .env file at {env_file.absolute()}...")
    
    with open(env_file, 'w') as f:
        f.write("# Optimal Inquiro configuration for 13900HX + 16GB RAM + RTX 4060 8GB\n")
        f.write("# Generated by setup_optimal.py\n\n")
        
        f.write("# Memory Management\n")
        f.write(f"DEFAULT_MEMORY_LIMIT={config['DEFAULT_MEMORY_LIMIT']}\n\n")
        
        f.write("# Text Processing\n")
        f.write(f"CHUNK_SIZE={config['CHUNK_SIZE']}\n")
        f.write(f"CHUNK_OVERLAP={config['CHUNK_OVERLAP']}\n\n")
        
        f.write("# Query Settings\n")
        f.write(f"DEFAULT_K={config['DEFAULT_K']}\n")
        f.write(f"DEFAULT_SCORE_THRESHOLD={config['DEFAULT_SCORE_THRESHOLD']}\n")
        f.write(f"DEFAULT_MAX_CONTEXT_LENGTH={config['DEFAULT_MAX_CONTEXT_LENGTH']}\n\n")
        
        f.write("# Models\n")
        f.write(f"OLLAMA_QUERY_MODEL={config['OLLAMA_QUERY_MODEL']}\n")
        f.write(f"OLLAMA_EMBEDDING_MODEL={config['OLLAMA_EMBEDDING_MODEL']}\n")
    
    print(f"   ‚úì .env file created with optimal settings")

def check_ollama_models(config):
    """Check if the recommended models are available."""
    import subprocess
    
    print("\nü§ñ Checking Ollama models...")
    
    try:
        # Check if ollama is available
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, check=True)
        available_models = result.stdout
        
        query_model = config['OLLAMA_QUERY_MODEL']
        embedding_model = config['OLLAMA_EMBEDDING_MODEL']
        
        missing_models = []
        
        if query_model not in available_models:
            missing_models.append(query_model)
        else:
            print(f"   ‚úì Query model '{query_model}' is available")
            
        if embedding_model not in available_models:
            missing_models.append(embedding_model)
        else:
            print(f"   ‚úì Embedding model '{embedding_model}' is available")
        
        if missing_models:
            print(f"\nüì• Missing models detected: {missing_models}")
            choice = input("Install missing models now? [Y/n]: ").strip().lower()
            
            if choice in ['', 'y', 'yes']:
                for model in missing_models:
                    print(f"   Installing {model}...")
                    subprocess.run(['ollama', 'pull', model], check=True)
                    print(f"   ‚úì {model} installed successfully")
            else:
                print("   ‚ö†Ô∏è  You'll need to install these models manually later")
        
        return True
        
    except subprocess.CalledProcessError:
        print("   ‚ùå Ollama is not running or not accessible")
        print("   Please start Ollama server and run this script again")
        return False
    except FileNotFoundError:
        print("   ‚ùå Ollama is not installed")
        print("   Please install Ollama from https://ollama.ai")
        return False

def run_configuration():
    """Run the Inquiro configuration with the new settings."""
    print("\n‚öôÔ∏è  Running Inquiro configuration...")
    
    try:
        # Import and run configuration
        sys.path.append(str(Path.cwd()))
        from core.config import main as config_main
        config_main()
        return True
    except Exception as e:
        print(f"   ‚ùå Configuration failed: {e}")
        return False

def show_next_steps():
    """Show the user what to do next."""
    print("\nüéâ Setup complete! Next steps:")
    print()
    print("1. **Start using Inquiro:**")
    print("   python inquiro_cli.py --help")
    print()
    print("2. **Ingest your first documents:**")
    print("   python inquiro_cli.py populate --path /path/to/your/documents")
    print()
    print("3. **Query your documents:**")
    print("   python inquiro_cli.py query \"Your question here\"")
    print()
    print("4. **Use the TUI interface:**")
    print("   python inquiro_tui.py")
    print()
    print("üìñ For detailed configuration options, see OPTIMAL_CONFIG.md")
    print("üîß To modify settings, edit the .env file or set environment variables")

def main():
    """Main setup function."""
    print("=" * 80)
    print("üîß INQUIRO OPTIMAL SETUP - Hardware Optimized Configuration")
    print("=" * 80)
    
    # Set optimal configuration
    config = set_optimal_config()
    
    # Create .env file
    create_env_file(config)
    
    # Check models
    models_ok = check_ollama_models(config)
    
    if models_ok:
        # Run configuration
        config_ok = run_configuration()
        
        if config_ok:
            show_next_steps()
        else:
            print("\n‚ö†Ô∏è  Configuration had issues. Please check the output above.")
    else:
        print("\n‚ö†Ô∏è  Please fix Ollama setup and run this script again.")

if __name__ == "__main__":
    main()
