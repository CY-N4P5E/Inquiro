# Inquiro Technical Stack Specification

## Project Objective

Inquiro is a complete, production-ready RAG sys### 7. Funcements ‚Äî Potential Extensions

| Feature | Implementation | Benefit |
|---------|---------------|---------|
| **PDF Highlighting** | Streamlit + PDF rendering | ‚Ä¢ Visual source attribution<br>‚Ä¢ Direct verification of information |
| **Response Logging** | Simple file-based storage | ‚Ä¢ Record question/answer history<br>‚Ä¢ Review previous research sessions |
| **Citation Generation** | Enhanced metadata extraction | ‚Ä¢ More detailed source referencing<br>‚Ä¢ Improved traceability |
| **Additional Formats** | Expanded document loaders | ‚Ä¢ Support for Word, HTML, Markdown<br>‚Ä¢ Code repository integration |
| **Custom Chunking** | Configurable chunking strategies | ‚Ä¢ Domain-specific text segmentation<br>‚Ä¢ Improved context preservation |
| **Evaluation Tools** | Basic metrics and test cases | ‚Ä¢ System performance assessment<br>‚Ä¢ Benchmarking RAG quality |ned to provide accurate, document-grounded responses to natural language questions. It functions as a research assistant for literature review, technical documentation, and knowledge management with an emphasis on privacy, performance, and factual accuracy.

## Development Strategy

1. **Core Functionality First**: Focus on a stable, robust core implementation before extending features
2. **CLI-based Interface**: Maintain a command-line interface for maximum flexibility and integration
3. **Progressive Enhancement**: Add features incrementally after thorough testing
4. **Privacy by Design**: Process all data locally without external API dependencies

## Technical Stack Components

### 1. LLM Backend ‚Äî Ollama

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **LLM Runtime** | [Ollama](https://ollama.com/) | ‚Ä¢ Simple, one-command interface<br>‚Ä¢ Local-first architecture for privacy<br>‚Ä¢ Cross-platform support (Windows, macOS, Linux)<br>‚Ä¢ Active development and strong community |
| **Primary Model** | `llama3:8b` | ‚Ä¢ Excellent reasoning capabilities<br>‚Ä¢ Manageable resource requirements (16GB RAM)<br>‚Ä¢ Strong performance on factual Q&A tasks<br>‚Ä¢ Open weights enabling local deployment |
| **Alternative Models** | `mistral` or `codellama:7b` | ‚Ä¢ Lower resource requirements<br>‚Ä¢ Faster response times<br>‚Ä¢ Specialized capabilities (code for codellama) |

**ADR 001: Local Model Deployment**
* **Context**: Need to process potentially sensitive documents
* **Decision**: Use locally-hosted models via Ollama instead of API services
* **Consequences**: Improved privacy and reduced costs at expense of some performance
* **Status**: Accepted

**Performance Tip**: Enable GPU acceleration with `ollama run --gpu llama3:8b` for significantly improved response times.

### 2. Embedding Model ‚Äî Text Representation

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **Primary Embedding** | `nomic-embed-text` (via Ollama) | ‚Ä¢ High-quality embeddings (1024 dimensions)<br>‚Ä¢ Local processing consistent with privacy goals<br>‚Ä¢ Strong semantic understanding<br>‚Ä¢ Efficient computation |
| **Alternative** | `bge-small-en-v1.5` (HuggingFace) | ‚Ä¢ Smaller model size<br>‚Ä¢ Compatible with sentence-transformers<br>‚Ä¢ Good performance on retrieval tasks |
| **Chunk Size** | 500 characters (~100-120 tokens) | ‚Ä¢ Balances context preservation with retrieval precision<br>‚Ä¢ Avoids embedding quality degradation<br>‚Ä¢ Optimized for technical content |
| **Chunk Overlap** | 50 characters (10%) | ‚Ä¢ Maintains context continuity<br>‚Ä¢ Prevents information loss at chunk boundaries |

**ADR 002: Embedding Selection**
* **Context**: Need high-quality semantic search capabilities
* **Decision**: Use `nomic-embed-text` as primary embedding model
* **Alternatives Considered**: OpenAI embeddings, BGE embeddings, all-MiniLM
* **Consequences**: Strong performance with complete privacy; some additional setup required
* **Status**: Accepted

### 3. Document Processing ‚Äî Loading and Chunking

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **PDF Processor** | `PyMuPDF` (via LangChain) | ‚Ä¢ High-quality text extraction<br>‚Ä¢ Preserves document structure<br>‚Ä¢ Handles complex PDFs including academic papers<br>‚Ä¢ Extracts metadata (page numbers, titles) |
| **Text Chunking** | `RecursiveCharacterTextSplitter` | ‚Ä¢ Intelligent chunking respecting semantics<br>‚Ä¢ Configurable size and overlap<br>‚Ä¢ Preserves context within chunks<br>‚Ä¢ Handles various languages |
| **Framework** | LangChain | ‚Ä¢ Comprehensive document processing tools<br>‚Ä¢ Active development and support<br>‚Ä¢ Integration with multiple vector stores<br>‚Ä¢ Flexible component architecture |

**ADR 003: Document Processing Framework**
* **Context**: Need reliable document processing for various PDF types
* **Decision**: Use LangChain with PyMuPDF for document processing
* **Alternatives Considered**: LlamaIndex, custom processing pipeline
* **Consequences**: Simpler implementation with some tradeoffs in specialized academic parsing
* **Status**: Accepted with regular review as LlamaIndex capabilities evolve

### 4. Vector Database ‚Äî FAISS

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **Vector DB** | FAISS | ‚Ä¢ Industry-standard vector similarity search<br>‚Ä¢ Highly optimized for performance<br>‚Ä¢ Minimal resource requirements<br>‚Ä¢ Complete offline functionality<br>‚Ä¢ Support for millions of vectors |
| **Index Type** | Flat L2 / Inner Product | ‚Ä¢ Exact search for maximum accuracy<br>‚Ä¢ Suitable for collections up to ~1M vectors<br>‚Ä¢ No approximation loss |
| **Persistence** | Local file storage | ‚Ä¢ Reliable disk-based persistence<br>‚Ä¢ No external database dependencies<br>‚Ä¢ Portable between systems |
| **Alternative** | ChromaDB | ‚Ä¢ Better metadata filtering capabilities<br>‚Ä¢ More complex setup<br>‚Ä¢ Slower performance |

**ADR 004: Vector Database Selection**
* **Context**: Need efficient similarity search for document embeddings
* **Decision**: Use FAISS for vector storage and retrieval
* **Alternatives Considered**: ChromaDB, Milvus, Pinecone
* **Consequences**: Superior performance and simplicity with some limitations in complex metadata filtering
* **Status**: Accepted

### 5. RAG Pipeline ‚Äî Integration Components

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **Retrieval Mechanism** | `vectordb.as_retriever()` | ‚Ä¢ Configurable search parameters<br>‚Ä¢ Integration with similarity score filtering<br>‚Ä¢ Support for top-k retrieval logic |
| **Prompt Engineering** | Custom templating | ‚Ä¢ Explicit grounding instructions<br>‚Ä¢ Context-focused response generation<br>‚Ä¢ Source attribution preservation |
| **Response Generation** | Direct LLM invocation | ‚Ä¢ Fine-grained control over prompt construction<br>‚Ä¢ Simplified debugging and tracing<br>‚Ä¢ Custom formatting of responses |
| **Search Parameters** | k=5 with no minimum threshold | ‚Ä¢ Optimal context window utilization<br>‚Ä¢ Balance between coverage and precision<br>‚Ä¢ Prevents over/under retrieval |

**ADR 005: RAG Implementation Approach**
* **Context**: Need reliable, inspectable RAG implementation
* **Decision**: Use direct integration rather than high-level chain abstractions
* **Alternatives Considered**: LangChain RetrievalQA chains, LlamaIndex query engines
* **Consequences**: More control at the cost of some additional code complexity
* **Status**: Accepted

### 6. User Interface ‚Äî CLI First Approach

| Component | Choice | Rationale |
|-----------|--------|-----------|
| **Primary Interface** | Command-line (CLI) | ‚Ä¢ Maximum flexibility<br>‚Ä¢ Scriptable for automation<br>‚Ä¢ Easier testing and debugging<br>‚Ä¢ Lower maintenance burden |
| **Interactive Mode** | Terminal-based research assistant | ‚Ä¢ Natural conversation flow<br>‚Ä¢ Session persistence<br>‚Ä¢ Accessible to technical users |
| **Future UI Options** | Streamlit, Gradio, or Flask | ‚Ä¢ Web-based interface for broader accessibility<br>‚Ä¢ Visualization of source documents<br>‚Ä¢ Interactive refinement of queries |

**ADR 006: Interface Strategy**
* **Context**: Need reliable interface that prioritizes functionality
* **Decision**: Implement CLI first with web UI as future enhancement
* **Alternatives Considered**: Starting with web interface, desktop application
* **Consequences**: Faster development and iteration at cost of initial user experience
* **Status**: Accepted for initial phase


üßº Don‚Äôt rush into UI until logic is stable.
üõ†Ô∏è STEP 7: Optional Extras (Future Enhancements)
| Feature              | Tool                    | Notes                                 |
| -------------------- | ----------------------- | ------------------------------------- |
| PDF highlighting     | LlamaIndex + Streamlit  | Show exact chunk sources              |
| Feedback loop        | Save Q\&A sessions      | For tuning or retrying queries        |
| Citation generation  | Embed doc metadata      | Show which doc/chunk supported answer |
| Whisper integration  | Ollama or local whisper | Transcribe lecture audio if needed    |
| Markdown/HTML ingest | Python + BeautifulSoup  | Expand sources beyond PDFs            |

## Development Environment Summary

| Component | Selected Technology | Benefits |
|-----------|---------------------|---------|
| **LLM** | `llama3:8b` via Ollama | Local execution, strong reasoning |
| **Embedding** | `nomic-embed-text` via Ollama | High-quality semantic vectors |
| **Vector DB** | FAISS | Industry-standard similarity search |
| **Document Processing** | PyMuPDF + LangChain | Superior PDF handling |
| **Chunking** | RecursiveCharacterTextSplitter | Context-aware text segmentation |
| **Framework** | LangChain | Comprehensive RAG component suite |
| **Development** | Python 3.8+ | Excellent AI/ML ecosystem |
| **Deployment** | Local first, containerizable | Privacy and portability |

## System Requirements

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| **CPU** | 4 cores | 8+ cores |
| **RAM** | 8GB | 16GB+ |
| **Storage** | 5GB + document storage | SSD preferred |
| **GPU** | Not required | CUDA-compatible (4GB+ VRAM) |
| **OS** | Windows 10+, macOS 12+, Linux | Any 64-bit OS |

For larger document collections or faster processing, scale RAM and CPU resources accordingly.
